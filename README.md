#  NLP Projects â€” Manish Kanuri

This repository showcases my **Natural Language Processing (NLP)** assignments and deep learning projects developed as part of my coursework and personal exploration. Each project focuses on a key NLP concept â€” from basic text preprocessing and tokenization to deep learningâ€“based text generation using RNNs, LSTMs, and BPE tokenizers.

---

## ðŸ“‚ Repository Structure

### Assignment 1 â€” Text Preprocessing & Tokenization
- **Files:**  
  - `NLP_Assignment_1.ipynb`  
  - `NLP_Assignment_1_Report.pdf`  
- **Description:**  
  Covers text preprocessing and tokenization using **NLTK**, including stemming, lemmatization, stopword removal, and frequency analysis.

---

### Assignment 2 â€” Feature Engineering & Text Classification
- **Files:**  
  - `NLP_Assignment_2_Task1.ipynb`  
  - `NLP_Assignment_2_Task2.ipynb`  
  - `NLP_Assignment_2_Task3.ipynb`  
  - `NLP_Assignment_2_Task4.ipynb`  
  - `NLP_Assignment_2_Report.pdf`  
- **Description:**  
  Focuses on **feature extraction techniques** like Bag-of-Words, TF-IDF, and n-grams, followed by text classification using Naive Bayes, Logistic Regression, and SVM models.

---

### Assignment 3 â€” Sentiment Analysis & POS Tagging
- **Files:**  
  - `NLP_Assignment_3.ipynb`  
  - `NLP_Assignment_3_Report.pdf`  
- **Description:**  
  Implements sentiment analysis on movie review data using **NLTK** and **scikit-learn**. Includes parts-of-speech tagging, confusion matrix visualization, and accuracy/F1-score evaluation.

---

### Assignment 4 â€” Recurrent Neural Networks for Text Prediction
- **Files:**  
  - `NLP_Assignment_4.ipynb`  
  - `NLP_Assignment_4_Report.pdf`  
- **Description:**  
  Explores **sequence modeling** with RNN, GRU, and LSTM architectures using **PyTorch**, evaluating model perplexity and text coherence across epochs.

---

### Text Generation â€” LSTM vs RNN
- **Files:**  
  - `Text_Generation_LSTM_vs_RNN.ipynb`  
- **Description:**  
  Character-level **text generation** comparing vanilla RNNs and LSTMs.  
  Trained on **Warren Buffettâ€™s annual letters**, generating realistic long-form financial text using temperature sampling.  
  Includes training visualizations, loss curves, and perplexity trends.

---

### Byte Pair Encoding (BPE) Tokenizer Implementation
- Implemented a **custom BPE tokenizer** from scratch for subword encoding.  
- Compared vocabulary compression efficiency and token merging behavior.  
- Prepared datasets compatible with GPT-style Transformer architectures.

---

##  Technologies & Tools

- **Languages:** Python  
- **Libraries:**  
  - NLTK â€” Tokenization, stemming, lemmatization, POS tagging  
  - scikit-learn â€” TF-IDF, text classification, metrics  
  - PyTorch â€” RNN, GRU, LSTM implementations  
  - NumPy, Pandas â€” Data preprocessing  
  - Matplotlib, Seaborn â€” Visualizations  

---

##  Key Learning Outcomes

- Built full NLP pipelines from preprocessing to model evaluation.  
- Gained experience with both **classical NLP (BoW, TF-IDF)** and **neural NLP (RNN, LSTM)**.  
- Implemented **character-level language models** and **custom tokenizers**.  
- Analyzed how encoding and architecture choices affect text generation quality.

---

##  How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/ManishKanuri/NLP-Projects.git
   cd NLP-Projects
